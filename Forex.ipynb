{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib  inline\n",
    "import pandas as pd\n",
    "import iexfinance as iex\n",
    "from datetime import datetime\n",
    "import quandl\n",
    "# start plotting\n",
    "from plotly import tools\n",
    "tools.set_credentials_file(username='profiler84', api_key='Jt8y8BVFAZRt6jSX6ehC')\n",
    "tools.set_config_file(world_readable=True, sharing='public')\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import xgboost\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import sklearn.preprocessing as prep\n",
    "import time\n",
    "import sklearn.feature_selection as fs\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import poly1d\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "import datetime\n",
    "pd.core.common.is_list_like = pd.api.types.is_list_like\n",
    "from pandas_datareader import data as pdr\n",
    "import fix_yahoo_finance as yf\n",
    "yf.pdr_override()\n",
    "from pykalman import KalmanFilter\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage as ndimage\n",
    "\n",
    "def customLoss(actual,pred):\n",
    "    alpha = 100\n",
    "    t = 0\n",
    "    if (actual * pred > 0):\n",
    "        t = abs(actual-pred)\n",
    "    else:\n",
    "        t = alpha*pred**2 - np.sign(actual)*pred + abs(actual)\n",
    "    return (t)\n",
    "\n",
    "def macd(close, n_fast=12, n_slow=26,  n_sign=9, fillna=True):\n",
    "    emafast = close.ewm(n_fast).mean()\n",
    "    emaslow = close.ewm(n_slow).mean()\n",
    "    macd = emafast - emaslow\n",
    "    if fillna:\n",
    "        macd = macd.fillna(0)\n",
    "    macdsig = macd.ewm(n_sign).mean()\n",
    "    if fillna:\n",
    "        macd = macd.fillna(0)\n",
    "        macdsig = macdsig.fillna(0)\n",
    "    return macd, macdsig\n",
    "\n",
    "def simple_returns(df):\n",
    "    df = df.pct_change().fillna(0)\n",
    "    return df\n",
    "\n",
    "def rsi(close, period=14, fillna=True):\n",
    "    diff = close.diff()\n",
    "    which_dn = diff < 0\n",
    "\n",
    "    up, dn = diff, diff*0\n",
    "    up[which_dn], dn[which_dn] = 0, -up[which_dn]\n",
    "\n",
    "    emaup = up.ewm(period).mean()\n",
    "    emadn = dn.ewm(period).mean()\n",
    "\n",
    "    rsi = 100 * emaup/(emaup + emadn)\n",
    "    if fillna:\n",
    "        rsi = rsi.fillna(50)\n",
    "    return pd.Series(rsi, name='rsi')\n",
    "\n",
    "def wavetrend(high,low,close,n1=10,n2=21):\n",
    "    ap = (high+low+close)/3\n",
    "    esa = close.ewm(n1).mean()\n",
    "    d = abs(ap - esa).ewm(n1).mean()\n",
    "    ci = (ap - esa) / (0.02 * d)\n",
    "    tci = ci.ewm(n2).mean()\n",
    "    \n",
    "    wt1=tci\n",
    "    wt2=wt1.rolling(n2).mean()\n",
    "    wt2=wt2.fillna(method ='backfill')\n",
    "    return wt1, wt2\n",
    "\n",
    "def log_returns(df):\n",
    "    \"\"\"Calculate logReturns of price series\"\"\"\n",
    "    log_ret = np.log(df) - np.log(df.shift(1))\n",
    "    return log_ret\n",
    "\n",
    "def tsi(close, r=25, s=13, fillna=True):\n",
    "    \"\"\"Calculate TSI Indicator of close price series\"\"\"\n",
    "    m = close - close.shift(1)\n",
    "    m1 = m.ewm(r).mean().ewm(s).mean()\n",
    "    m2 = abs(m).ewm(r).mean().ewm(s).mean()\n",
    "    tsi = m1/m2\n",
    "    if fillna:\n",
    "        tsi = tsi.fillna(0)\n",
    "    return pd.Series(100*tsi, name='tsi')\n",
    "\n",
    "def on_balance_volume(close, volume):\n",
    "    \"\"\"Calculate OBV Indicator of close price series and Volume\"\"\"\n",
    "    df = pd.DataFrame([close, volume]).transpose()\n",
    "    df['OBV'] = 0\n",
    "    c1 = close < close.shift(1)\n",
    "    c2 = close > close.shift(1)\n",
    "    if c1.any():\n",
    "        df.loc[c1, 'OBV'] = - volume\n",
    "    if c2.any():\n",
    "        df.loc[c2, 'OBV'] = volume\n",
    "    return df['OBV']\n",
    "\n",
    "def average_true_range(high, low, close, period=14, fillna=False):\n",
    "    \"\"\"Average True Range (ATR)\n",
    "    The indicator provide an indication of the degree of price volatility.\n",
    "    Strong moves, in either direction, are often accompanied by large ranges,\n",
    "    or large True Ranges.\n",
    "    http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:average_true_range_atr\n",
    "    Args:\n",
    "        high(pandas.Series): dataset 'High' column.\n",
    "        low(pandas.Series): dataset 'Low' column.\n",
    "        close(pandas.Series): dataset 'Close' column.\n",
    "        n(int): n period.\n",
    "    Returns:\n",
    "        pandas.Series: New feature generated.\n",
    "    \"\"\"\n",
    "    cs = close.shift(1)\n",
    "    tr = high.combine(cs, max) - low.combine(cs, min)\n",
    "    tr = tr.ewm(period).mean()\n",
    "    if fillna:\n",
    "        tr = tr.fillna(0)\n",
    "    return pd.Series(tr, name='atr')\n",
    "\n",
    "def kst(close, r1=10, r2=15, r3=20, r4=30, n1=10, n2=10, n3=10, n4=15, nsig=9, fillna=True):\n",
    "    \"\"\"Calculate KST Indicator of close price series. See https://en.wikipedia.org/wiki/KST_oscillator\"\"\"\n",
    "    rocma1 = (close / close.shift(r1) - 1).rolling(n1).mean()\n",
    "    rocma2 = (close / close.shift(r2) - 1).rolling(n2).mean()\n",
    "    rocma3 = (close / close.shift(r3) - 1).rolling(n3).mean()\n",
    "    rocma4 = (close / close.shift(r4) - 1).rolling(n4).mean()\n",
    "    kst = 100*(rocma1 + 2*rocma2 + 3*rocma3 + 4*rocma4)\n",
    "    sig = kst.rolling(nsig).mean()\n",
    "    if fillna:\n",
    "        sig = sig.fillna(0)\n",
    "    return pd.Series(sig, name='sig')\n",
    "\n",
    "def find_nearest(array, value,findMax=True):\n",
    "    \"\"\"Find nearest value of extreme value\"\"\"\n",
    "    array = np.sort(np.asarray(array))\n",
    "    if findMax:\n",
    "        array = array[array>=value]\n",
    "    else:\n",
    "        array = array[array<=value]\n",
    "    if array.size > 0:\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return array[idx]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def local_extreme(array, min_distance = 20, periodic=False, edges_allowed=True,find_max=True): \n",
    "    \"\"\"Find all local maxima of the array, separated by at least min_distance.\"\"\"\n",
    "    array = np.asarray(array)\n",
    "    cval = 0 \n",
    "    if periodic: \n",
    "        mode = 'wrap' \n",
    "    elif edges_allowed: \n",
    "        mode = 'nearest' \n",
    "    else: \n",
    "        mode = 'constant' \n",
    "    cval = array.max()+1 \n",
    "    if find_max:\n",
    "        max_points = array == ndimage.maximum_filter(array, 1+2*min_distance, mode=mode, cval=cval) \n",
    "    else:\n",
    "        max_points = array == ndimage.minimum_filter(array, 1+2*min_distance, mode=mode, cval=cval) \n",
    "        \n",
    "    return [indices[max_points] for indices in np.indices(array.shape)]\n",
    "\n",
    "def extreme_significance(exArray, tolerance=0.02,signi_level=2):\n",
    "    \"\"\"Find Significance of extreme values within tolerance bound.\"\"\"\n",
    "    array = np.asarray(exArray)\n",
    "    array = np.ravel(array)\n",
    "    signifArr = np.zeros(len(array))\n",
    "    d = {'data': array, 'sign': signifArr}\n",
    "    result = pd.DataFrame(data=d).values\n",
    "    #print('len of x is: '+repr(len(result)))\n",
    "    for i in range(len(result)):\n",
    "        val = result[i,0]\n",
    "        signif = 0\n",
    "        for j in range(i+1,len(result)):\n",
    "            if (result[j,0]>= val*(1-tolerance)) and  (array[j]<= val*(1+tolerance)):\n",
    "                    signif = signif + 1\n",
    "            result[i,1] = signif\n",
    "            \n",
    "    result = pd.DataFrame(data=result,columns=['ExtremeValue', 'Significance'])\n",
    "    result = result[result.Significance>=signi_level]\n",
    "    return result.sort_values(by=['Significance'],ascending=False)\n",
    "\n",
    "def grouper(iterable, tolerance):\n",
    "    \"\"\"Group extremes by cluster based on relative price distance.\"\"\"\n",
    "    # if Dataframe take first column\n",
    "    if isinstance(iterable,pd.core.frame.DataFrame):\n",
    "        iterable = iterable.sort_values(by=iterable.columns[0]).iloc[:,0].values\n",
    "    elif isinstance(iterable,np.ndarray):\n",
    "        iterable =np.sort(iterable)\n",
    "    prev = None\n",
    "    group = []\n",
    "    for item in iterable:\n",
    "        if not prev or item - prev <= tolerance*item:\n",
    "            group.append(item)\n",
    "        else:\n",
    "            yield group\n",
    "            group = [item]\n",
    "        prev = item\n",
    "    if group:\n",
    "        yield group\n",
    "def group_extremes(inp,max_distance):\n",
    "    \"\"\"Calculate centroid for each cluster of prices\"\"\"\n",
    "    inp = dict(enumerate(grouper(inp,max_distance), 1))\n",
    "    avgDict = {}\n",
    "    for k,v in inp.items():\n",
    "        # v is the list of items for dict row k\n",
    "        avgDict[k] = sum(v)/ float(len(v))\n",
    "    return list(avgDict.values())\n",
    "\n",
    "def highest_high(df,period=21):\n",
    "    df = df.rolling(window=period,center=False,min_periods=1).max()\n",
    "    return df\n",
    "\n",
    "def momentum(df,period=10):\n",
    "    df = df - df.shift(period)\n",
    "    df[0:period] = 0\n",
    "    return df\n",
    "\n",
    "def trading_range(df_high,df_low):\n",
    "    df = df_high-df_low\n",
    "    return df\n",
    "\n",
    "def lowest_low(df,period=21):\n",
    "    df = df.rolling(window=period,center=False,min_periods=1).min()\n",
    "    return df\n",
    "\n",
    "def calc_extreme_distance(series,sup_array,res_array):\n",
    "    import math\n",
    "    if isinstance(series,pd.core.frame.DataFrame):\n",
    "        series = np.ravel(series.values())\n",
    "    elif isinstance(series,np.ndarray):\n",
    "        series =np.ravel(series)\n",
    "    sup_array = np.ravel(np.array(sup_array))\n",
    "    res_array = np.ravel(np.array(res_array))\n",
    "    dist_s = []\n",
    "    dist_r = []\n",
    "    #if not sup_array:\n",
    "    #    print(\"test\")\n",
    "    #else:\n",
    "    for i in range(len(series)):\n",
    "        curr_value = series[i]\n",
    "        nearest_sup = find_nearest(sup_array, curr_value,findMax=False)\n",
    "        nearest_res = find_nearest(res_array, curr_value)\n",
    "        if curr_value*1.1 < nearest_sup:\n",
    "            dist_s.append(curr_value/2)\n",
    "        elif curr_value > nearest_res*1.1:\n",
    "            dist_r.append(curr_value/2)\n",
    "        else:\n",
    "            sup_val = curr_value-nearest_sup\n",
    "            res_val = nearest_res-curr_value\n",
    "            if np.isnan(sup_val):\n",
    "                sup_val = (curr_value/2)\n",
    "            if np.isnan(res_val):\n",
    "                res_val = (curr_value/2)\n",
    "            dist_s.append(sup_val)\n",
    "            dist_r.append(res_val)\n",
    "    if dist_s and dist_r:\n",
    "        return dist_s,dist_r\n",
    "    \n",
    "def r_score(y_true, y_pred, sample_weight=None, multioutput=None):\n",
    "    r2 = r2_score(y_true, y_pred, sample_weight=sample_weight, multioutput=multioutput)\n",
    "    r = (np.sign(r2)*np.sqrt(np.abs(r2)))\n",
    "    if r <= -1:\n",
    "        return -1\n",
    "    else:\n",
    "        return r\n",
    "    \n",
    "def calc_r_score (actual_and_test):\n",
    "    r = []\n",
    "    r.append(0)\n",
    "    for i in range(1,len(actual_and_test)):\n",
    "        ri = r_score(actual_and_test[0:i,0],actual_and_test[0:i,1])\n",
    "        if ri <= -1:\n",
    "            ri = 0\n",
    "        r.append(ri)\n",
    "    return r\n",
    "\n",
    "def calc_kalman(dataframe):\n",
    "    if isinstance(dataframe,pd.core.frame.DataFrame) or isinstance(dataframe,pd.core.series.Series):\n",
    "        array = dataframe.values\n",
    "    else:\n",
    "        array = dataframe\n",
    "    kf = KalmanFilter(transition_matrices = [1],\n",
    "                      observation_matrices = [1],\n",
    "                      initial_state_mean = 0,\n",
    "                      initial_state_covariance = 1,\n",
    "                      observation_covariance=1,\n",
    "                      transition_covariance=.01)\n",
    "    state_means, _ = kf.filter(array)\n",
    "    state_means = state_means.flatten()\n",
    "    state_means = pd.DataFrame(data=state_means,index=dataframe.index)\n",
    "    state_means.columns = ['KF']\n",
    "    state_means[\"KF\"][0:9]  = float(np.mean(state_means[9:15]).values)\n",
    "    return state_means\n",
    "\n",
    "def calc_extreme_groups (dataframe,find_max=True,mDistance = 100,tolerance=0.0009,signi_level=1):\n",
    "    \"\"\"Group extremes by cluster based on absolute price distance.\n",
    "       Input: series or dataframe\n",
    "       find_max: maximum (resistance) or minimum (suppport) search\n",
    "       mDistance: minimumdistance in an array to find local extremes\n",
    "       tolerance: tolerance level in percent to group other extremes\n",
    "       signi_level: leave 1 \"\"\"\n",
    "    if isinstance(dataframe,pd.core.frame.DataFrame) or isinstance(dataframe,pd.core.series.Series):\n",
    "        array = np.ravel(dataframe.values)     \n",
    "    array = np.ravel(array[local_extreme(array,min_distance= mDistance,find_max=find_max)])\n",
    "    signify =extreme_significance(array, tolerance=tolerance,signi_level=signi_level)\n",
    "    extreme = group_extremes(signify,tolerance)\n",
    "    return array,signify,extreme\n",
    "\n",
    "def calc_next_extreme_distance (dataframe,extremes_sup=None,extremes_res=None):\n",
    "    \"\"\"Given a series and their supports and resistances\n",
    "    calculate the distances to the next supports or resistances.\"\"\"\n",
    "    if isinstance(dataframe,pd.core.frame.DataFrame) or isinstance(dataframe,pd.core.series.Series):\n",
    "        array = np.ravel(dataframe.values) \n",
    "    [sup,res] = calc_extreme_distance(array,extremes_sup,extremes_res) \n",
    "    sr = pd.DataFrame(data=np.transpose([sup,res]),index=dataframe.index )\n",
    "    sr.columns = ['support_distance', 'resistance_distance']\n",
    "    return sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EURUSD = pd.read_csv('EURUSD.txt',delimiter=',',dtype={'<TIME>': str})\n",
    "EURUSD.columns = EURUSD.columns.str.replace(\"<\", \"\")\n",
    "EURUSD.columns = EURUSD.columns.str.replace(\">\", \"\")\n",
    "EURUSD = EURUSD.drop(['VOL'], axis=1)\n",
    "EURUSD['DATETIME'] = EURUSD['DTYYYYMMDD'].astype(str) + \":\" + EURUSD['TIME'].astype(str)\n",
    "EURUSD = EURUSD.drop(['DTYYYYMMDD','TIME'], axis=1)\n",
    "EURUSD['DATETIME']  = pd.to_datetime(EURUSD['DATETIME'],format='%Y%m%d:%H%M%S',  infer_datetime_format=False)\n",
    "EURUSD.index = pd.to_datetime(EURUSD['DATETIME'],format='%Y%m%d:%H%M%S',  infer_datetime_format=False, unit=\"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EURUSD_1H = EURUSD.resample('1H').agg({'OPEN': 'first', \n",
    "                                 'HIGH': 'max', \n",
    "                                 'LOW': 'min', \n",
    "                                 'CLOSE': 'last'})\n",
    "\n",
    "EURUSD_4H = EURUSD.resample('4H').agg({'OPEN': 'first', \n",
    "                                 'HIGH': 'max', \n",
    "                                 'LOW': 'min', \n",
    "                                 'CLOSE': 'last'})\n",
    "\n",
    "EURUSD_D = EURUSD.resample('1D').agg({'OPEN': 'first',\n",
    "                                 'HIGH': 'max', \n",
    "                                 'LOW': 'min', \n",
    "                                 'CLOSE': 'last'})\n",
    "EURUSD_D = EURUSD_D.dropna().pct_change().dropna()\n",
    "EURUSD_D['Weekday'] = EURUSD_D.index.weekday\n",
    "EURUSD_D['Month'] = EURUSD_D.index.month\n",
    "EURUSD_D['DayMonth'] = EURUSD_D.index.day\n",
    "\n",
    "EURUSD_1H = EURUSD_1H.dropna()\n",
    "EURUSD_4H = EURUSD_4H.dropna()\n",
    "\n",
    "EURUSD_M = EURUSD_1H.resample('M').agg({'OPEN': 'first',\n",
    "                                 'HIGH': 'max', \n",
    "                                 'LOW': 'min', \n",
    "                                 'CLOSE': 'last'})\n",
    "EURUSD_M = EURUSD_M.dropna().pct_change().dropna()\n",
    "EURUSD_M['Month'] = EURUSD_M.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl=np.ravel(EURUSD_1H[['CLOSE']].values)\n",
    "plt.rc(\"figure\",figsize=(16,16))\n",
    "plt.plot(cl,color='xkcd:blue',linestyle ='-')\n",
    "mDistance = 1000\n",
    "plt.plot(np.ravel(local_extreme(cl,min_distance= mDistance,find_max=True)),\n",
    "         np.ravel(cl[local_extreme(cl,min_distance= mDistance,find_max=True)]) ,color='xkcd:green',marker='o',linestyle ='')\n",
    "plt.plot(np.ravel(local_extreme(cl,min_distance= mDistance,find_max=False)),\n",
    "         np.ravel(cl[local_extreme(cl,min_distance= mDistance,find_max=False)]) ,color='xkcd:red',marker='o',linestyle ='')\n",
    "plt.grid(True)\n",
    "plt.title(\"Local extremes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance= 0.001\n",
    "signi_level=3\n",
    "min_distance= 180\n",
    "array = np.ravel(cl[local_extreme(cl,min_distance= min_distance,find_max=False)])\n",
    "#signify =extreme_significance(array, tolerance=tolerance,signi_level=signi_level)\n",
    "signify =extreme_significance(array, tolerance=tolerance,signi_level=signi_level)\n",
    "extreme = group_extremes(signify,tolerance)\n",
    "print('localextremes:' + repr(len(array)))\n",
    "print('signify:' + repr(len(signify)), ' signimax:' +repr(np.max(signify['Significance'])))\n",
    "print('extremes:' + repr(len(extreme)))\n",
    "      \n",
    "_,_,extremes_res = calc_extreme_groups(EURUSD_1H[['CLOSE']],signi_level=signi_level,\n",
    "                                   tolerance=tolerance,mDistance = min_distance,find_max=True)\n",
    "plt.rc(\"figure\",figsize=(16,16))\n",
    "plt.plot(cl,color='xkcd:blue',linestyle ='-')\n",
    "yval = np.ravel(extremes_res)\n",
    "for x in yval:\n",
    "    plt.axhline(y=x,color='xkcd:red',linestyle =':')\n",
    "plt.grid(True)\n",
    "plt.title(\"resistances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show horizontal resistance levels based on extreme points and significance\n",
    "__,__,extremes_sup = calc_extreme_groups(EURUSD_1H[['CLOSE']],signi_level=signi_level,\n",
    "                                   tolerance=tolerance,mDistance = min_distance,find_max=False)\n",
    "plt.rc(\"figure\",figsize=(16,16))\n",
    "plt.plot(cl,color='xkcd:blue',linestyle ='-')\n",
    "yval = np.ravel(extremes_sup)\n",
    "print('localextremes:' + repr(len(array)))\n",
    "print('signify:' + repr(len(signify)), ' signimax:' +repr(np.max(signify['Significance'])))\n",
    "print('extremes:' + repr(len(extreme)))\n",
    "for x in yval:\n",
    "    plt.axhline(y=x,color='xkcd:green',linestyle =':')\n",
    "plt.grid(True)\n",
    "plt.title(\"Supports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyti.stochrsi import stochrsi as stochrsi\n",
    "from pyti.commodity_channel_index import commodity_channel_index as cci\n",
    "\n",
    "def feature_engineering_dates(df):\n",
    "    import pandas as pd\n",
    "    from datetime import timedelta\n",
    "    #df['Date'] = pd.to_datetime(\"'2015-10-01'\") # I guess this date is right :P\n",
    "    #df['Date'] = df['DATETIME'] + df['Day'].map(timedelta) - timedelta(days=1)\n",
    "    df['Year'] = df.index.year\n",
    "    df['Month'] = df.index.month\n",
    "    df['Week'] = df.index.weekofyear\n",
    "    df['Weekday'] = df.index.weekday\n",
    "    df['DayMonth'] = df.index.day\n",
    "    return df\n",
    "\n",
    "def feature_engineering_ta(df):\n",
    "        # bollinger bands\n",
    "    df['TSI'] = tsi(df['CLOSE'])\n",
    "    df['ATR'] = average_true_range(df['HIGH'],df['LOW'],df['CLOSE'],period=14)\n",
    "    df['WT1'],df['WT2'] = wavetrend(df['HIGH'],df['LOW'],df['CLOSE'],n1=10,n2=21)\n",
    "    df['KF'] = calc_kalman(df['CLOSE'])\n",
    "    df['KST'] = kst(df['CLOSE'])\n",
    "    df['HIGHESTHIGH'] = highest_high(df['HIGH'],period=21)\n",
    "    df['LOWESTLOW'] = lowest_low(df['LOW'],period=21)\n",
    "    df['TRADINGRANGE'] = trading_range(df['HIGH'],df['LOW'])\n",
    "    df['MOMENTUM'] = momentum(df['CLOSE'],period=10)\n",
    "    df['RSI'] = rsi(df['CLOSE'],period=14)\n",
    "    df['MACD'],df['MACDSIGNAL'] = macd(df['CLOSE'])\n",
    "    df['STOCHRSI'] = stochrsi(df['CLOSE'], 14)\n",
    "    df['CCI'] = cci(df['CLOSE'],df['HIGH'],df['LOW'], 20)\n",
    "    return df\n",
    "\n",
    "def feature_engineering_statistics(df_grouped,cols):\n",
    "    for c in cols:\n",
    "        # rolling\n",
    "        for h in [11, 21]:\n",
    "            df_grouped[c+'_rolling_mean_'+str(h)] = df_grouped[c].rolling(h, min_periods=0).mean()\n",
    "        # diffs\n",
    "        df_grouped[c+'_diff_1'] = df_grouped[c].diff().fillna(method=\"backfill\")\n",
    "        df_grouped[c+'_diff_2'] = df_grouped[c].diff(2).fillna(method=\"backfill\")\n",
    "        df_grouped[c+'_diff_3'] = df_grouped[c].diff(3).fillna(method=\"backfill\")\n",
    "        # cumsum\n",
    "        df_grouped[c+'_cumsum'] = df_grouped[c].cumsum()\n",
    "        # shift columns\n",
    "        df_grouped[c+'_shift1'] = df_grouped[c].shift(1).fillna(0)\n",
    "        df_grouped[c+'_shift2'] = df_grouped[c].shift(2).fillna(0)\n",
    "        df_grouped[c+'_shift3'] = df_grouped[c].shift(3).fillna(0)\n",
    "    return df_grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EURUSD_1H = feature_engineering_dates(EURUSD_1H)\n",
    "EURUSD_1H = feature_engineering_ta(EURUSD_1H)\n",
    "EURUSD_1H = EURUSD_1H.fillna(method='bfill')\n",
    "\n",
    "cols = ['TSI','ATR','KF','KST','HIGHESTHIGH','LOWESTLOW','TRADINGRANGE','MOMENTUM','RSI','MACD','MACDSIGNAL','WT1','WT2','STOCHRSI','CCI']\n",
    "#EURUSD_1H = feature_engineering_statistics(EURUSD_1H,cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot example for wave trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = EURUSD_1H.index.searchsorted(datetime.datetime(2001, 1, 22,0))\n",
    "end = EURUSD_1H.index.searchsorted(datetime.datetime(2001, 2, 25))\n",
    "EURUSD_temp =EURUSD_1H.iloc[start:end]\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "data = go.Candlestick(x=EURUSD_temp.index, open=EURUSD_temp['OPEN'],high=EURUSD_temp['HIGH'],low=EURUSD_temp['LOW'],\n",
    "                      close=EURUSD_temp['CLOSE'])\n",
    "\n",
    "wt1 = go.Scatter(\n",
    "    x=EURUSD_temp.index,\n",
    "    y=EURUSD_temp['WT1'],\n",
    "    name = 'WT1'\n",
    ")\n",
    "\n",
    "wt2 = go.Scatter(\n",
    "    x=EURUSD_temp.index,\n",
    "    y=EURUSD_temp['WT2'],\n",
    "    name = 'WT2'\n",
    ")\n",
    "wt3 = go.Scatter(\n",
    "    x=EURUSD_temp.index,\n",
    "    y=EURUSD_temp['STOCHRSI'],\n",
    "    name = 'STOCHRSI'\n",
    ")\n",
    "#py.plotly.iplot(wt,filename='basic_candle', auto_open=True)\n",
    "fig = tools.make_subplots(rows=2, cols=1)\n",
    "fig.append_trace(data, 1, 1)\n",
    "fig.append_trace(wt1, 2, 1)\n",
    "fig.append_trace(wt2, 2, 1)\n",
    "#fig.append_trace(wt3, 2, 1)\n",
    "py.plotly.iplot(fig,filename='basic_candle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_profit_loss(i=None,data=None,profit=0.01):\n",
    "    start = data['CLOSE'][i]\n",
    "    starttime = data.index[i]\n",
    "    if (i+1)>len(data):\n",
    "        return\n",
    "    search = data[i+1:]\n",
    "    longvalue = float('NaN')\n",
    "    shortvalue = float('NaN')\n",
    "    try:\n",
    "        longtime = search[search['HIGH']>=start*(1+profit)].index[0]\n",
    "        longvalue = search[search['HIGH']>=start*(1+profit)]\n",
    "        longvalue = longvalue['HIGH'][0]\n",
    "    except Exception as e:\n",
    "        ts = time.time()\n",
    "        longtime = datetime.datetime.fromtimestamp(ts)\n",
    "    try:\n",
    "        shorttime = search[search['LOW']<= start*(1-profit)].index[0]\n",
    "        shortvalue = search[search['LOW']<= start*(1-profit)]\n",
    "        shortvalue = shortvalue['LOW'][0]\n",
    "    except Exception as e:\n",
    "        ts = time.time()\n",
    "        shorttime = datetime.datetime.fromtimestamp(ts)\n",
    "    #print( \"longtime=\" + str(longtime) + \",shorttime=\" + str(shorttime))\n",
    "    \n",
    "    if longtime<shorttime:\n",
    "        return starttime,longtime,1,(longtime-starttime).seconds/3600,start,longvalue\n",
    "    elif longtime>shorttime:\n",
    "        return starttime,shorttime,-1,(shorttime-starttime).seconds/3600,start,shortvalue\n",
    "    elif longtime.date()==shorttime.date() and longtime.minute == shorttime.minute:\n",
    "        return float('NaN'),float('NaN'),float('NaN'),float('NaN'),float('NaN'),float('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profit = 0.01 # in EURUSD Notation\n",
    "data = EURUSD_1H[['HIGH','LOW','CLOSE']]\n",
    "start = data['CLOSE'][1]\n",
    "starttime = data.index[1]\n",
    "search = data[2:]\n",
    "longvalue = search[search['HIGH']>=start*(1+profit)]\n",
    "longvalue = longvalue['HIGH'][0]\n",
    "search[search['HIGH']>=start*(1+profit)].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "profit = 0.01 # in EURUSD Notation\n",
    "data = EURUSD_1H[['HIGH','LOW','CLOSE']]\n",
    "start = time.time()\n",
    "inputs = range(len(data))\n",
    "\n",
    "results = Parallel(n_jobs=4,verbose=1)(delayed(calc_profit_loss)(x, data, profit) for x in inputs)\n",
    "end = time.time()\n",
    "print('time elapsed: ' + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [x for x in results if x is not None]\n",
    "results = [x for x in results if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [x for x in results if x is not None]\n",
    "df = pd.DataFrame(results, columns=['pos','endtime','signal', 'time_diff_hours','start_price','end_price'])\n",
    "df.index = df['pos']\n",
    "df = df.drop(['pos'], axis=1)\n",
    "\n",
    "data_merged = EURUSD_1H.merge(df, left_index=True,right_index =True, how='left', indicator=True)\n",
    "data_merged = data_merged.dropna()\n",
    "data_merged = data_merged.drop(['_merge'], axis=1)\n",
    "data_merged = data_merged[data_merged['time_diff_hours'] !=0]\n",
    "data_merged['pct_change'] = (data_merged['end_price']/data_merged['start_price'])-1\n",
    "data_merged['weight'] = 1/data_merged['time_diff_hours']\n",
    "data_merged['weighted_target'] = data_merged['weight']*data_merged['pct_change']\n",
    "data_merged.to_csv('EURUSD_1H_signal_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonality in the performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean weekday performance strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = data_merged.groupby(['Weekday'])['pct_change'].mean()\n",
    "plt.rc(\"figure\",figsize=(10,3))\n",
    "plt.bar(bars.index,bars)\n",
    "plt.grid()\n",
    "plt.title('Bar 6 = Sunday 23:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean weekday performance day to day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = EURUSD_D.groupby(['Weekday'])['CLOSE'].mean()\n",
    "plt.rc(\"figure\",figsize=(10,3))\n",
    "plt.bar(bars.index,bars)\n",
    "plt.grid()\n",
    "plt.title('Bar 6 = Sunday 23:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sum weekday signal strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = data_merged.groupby(['Weekday'])['signal'].sum()\n",
    "plt.rc(\"figure\",figsize=(10,3))\n",
    "plt.bar(bars.index,bars)\n",
    "plt.grid()\n",
    "plt.title('Bar 6 = Sunday 23:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean daymonth performance strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = data_merged.groupby(['DayMonth'])['pct_change'].mean()\n",
    "plt.rc(\"figure\",figsize=(16,3))\n",
    "plt.bar(bars.index,bars)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean daymonth performance day to day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = EURUSD_D.groupby(['DayMonth'])['CLOSE'].mean()\n",
    "plt.rc(\"figure\",figsize=(16,3))\n",
    "plt.bar(bars.index,bars)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sum daymonth signal strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = data_merged.groupby(['DayMonth'])['signal'].sum()\n",
    "plt.rc(\"figure\",figsize=(16,3))\n",
    "plt.bar(bars.index,bars)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sum month signal strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = data_merged.groupby(['Month'])['signal'].sum()\n",
    "plt.rc(\"figure\",figsize=(16,3))\n",
    "plt.bar(bars.index,bars)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean month performance month to month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = EURUSD_M.groupby(['Month'])['CLOSE'].mean()\n",
    "plt.rc(\"figure\",figsize=(16,3))\n",
    "plt.bar(bars.index,bars)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean month performance strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = data_merged.groupby(['Month'])['pct_change'].mean()\n",
    "plt.rc(\"figure\",figsize=(16,3))\n",
    "plt.bar(bars.index,bars)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_merged.head(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged[data_merged['time_diff_hours'] ==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_merged\n",
    "# Now let us look at the correlation coefficient of each of these variables\n",
    "x_cols = [col for col in df.columns if col not in ['OPEN','HIGH','LOW','CLOSE','start_price','end_price',\n",
    "                                                   'time_diff_hours','pct_change','weighted_target','signal','weight']]\n",
    "\n",
    "target_col = 'signal'\n",
    "labels = []\n",
    "values = []\n",
    "for col in x_cols:\n",
    "    labels.append(col)\n",
    "    values.append(np.corrcoef(df[col].values, df[target_col].values)[0,1])\n",
    "    \n",
    "ind = np.arange(len(labels))\n",
    "width = 0.9\n",
    "fig, ax = plt.subplots(figsize=(16,16))\n",
    "plt.grid(True)\n",
    "rects = ax.barh(ind, np.array(values), color='y')\n",
    "ax.set_yticks(ind+((width)/2.))\n",
    "ax.set_yticklabels(labels, rotation='horizontal')\n",
    "ax.set_xlabel(\"Correlation coefficient\")\n",
    "ax.set_title(\"Correlation coefficient\")\n",
    "#autolabel(rects)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# trading rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "def plot_trade(i):\n",
    "    i = int(i)\n",
    "    EURUSD_temp = data_merged.iloc[i:int(1+i+data_merged['time_diff_hours'][i])]\n",
    "    # start plotting\n",
    "    from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "    from plotly import tools\n",
    "    py.tools.set_credentials_file(username='profiler84', api_key='Jt8y8BVFAZRt6jSX6ehC')\n",
    "    py.tools.set_config_file(world_readable=True, sharing='public')\n",
    "    init_notebook_mode(connected=True)\n",
    "    data = [go.Candlestick(x=EURUSD_temp.index, open=EURUSD_temp['OPEN'],high=EURUSD_temp['HIGH'],low=EURUSD_temp['LOW'],\n",
    "                          close=EURUSD_temp['CLOSE'])]\n",
    "    py.offline.iplot(data,filename='basic_candle2')\n",
    "    \n",
    "interact(plot_trade, i='1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signalgenerator_wavetrend(df):\n",
    "    time = []\n",
    "    signal = []\n",
    "    pos = False\n",
    "    long = False\n",
    "    for i in range(len(df)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        # Entry\n",
    "        if (df['WT1'][i-1]<df['WT2'][i-1]) and (df['WT1'][i]>=df['WT2'][i]) and (df['WT1'][i] <=-45):\n",
    "            time.append(df.index[i])\n",
    "            pos = True\n",
    "            long = True\n",
    "            signal.append(1)\n",
    "            continue\n",
    "        elif (df['WT1'][i-1]>df['WT2'][i-1]) and (df['WT1'][i]<=df['WT2'][i]) and (df['WT1'][i] >=45):\n",
    "            time.append(df.index[i])\n",
    "            pos = True\n",
    "            long = False\n",
    "            signal.append(-1)\n",
    "            continue\n",
    "        #if pos and long and (df['WT1'][i-1]>df['WT2'][i-1]) and (df['WT1'][i]<=df['WT2'][i]):\n",
    "        #    pos = False\n",
    "        #    time.append(df.index[i])\n",
    "        #    signal.append(0)\n",
    "        #if pos and not long and (df['WT1'][i-1]<df['WT2'][i-1]) and (df['WT1'][i]>=df['WT2'][i]):\n",
    "        #    pos = False\n",
    "        #    time.append(df.index[i])\n",
    "        #    signal.append(0)\n",
    "            \n",
    "    d = {'time': time, 'wavetrend_signal': signal}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df.index = df['time']\n",
    "    df = df.drop(['time'],axis=1)\n",
    "    return df\n",
    "\n",
    "def signalgenerator_stochrsi(df):\n",
    "    time = []\n",
    "    signal = []\n",
    "    pos = False\n",
    "    long = False\n",
    "    for i in range(len(df)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        # Entry\n",
    "        if (df['STOCHRSI'][i-1]<df['STOCHRSI'][i]) and (df['STOCHRSI'][i] <=10) and (df['STOCHRSI'][i-2]<df['STOCHRSI'][i-1]):\n",
    "            time.append(df.index[i])\n",
    "            pos = True\n",
    "            long = True\n",
    "            signal.append(1)\n",
    "            continue\n",
    "        elif (df['STOCHRSI'][i-1]>df['STOCHRSI'][i]) and (df['STOCHRSI'][i] >=90) and (df['STOCHRSI'][i-2]>df['STOCHRSI'][i-1]):\n",
    "            time.append(df.index[i])\n",
    "            pos = True\n",
    "            long = False\n",
    "            signal.append(-1)\n",
    "            continue\n",
    "        #if pos and long and (df['WT1'][i-1]>df['WT2'][i-1]) and (df['WT1'][i]<=df['WT2'][i]):\n",
    "        #    pos = False\n",
    "        #    time.append(df.index[i])\n",
    "        #    signal.append(0)\n",
    "        #if pos and not long and (df['WT1'][i-1]<df['WT2'][i-1]) and (df['WT1'][i]>=df['WT2'][i]):\n",
    "        #    pos = False\n",
    "        #    time.append(df.index[i])\n",
    "        #    signal.append(0)\n",
    "            \n",
    "    d = {'time': time, 'stochrsi_signal': signal}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df.index = df['time']\n",
    "    df = df.drop(['time'],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wavetrend generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_signal = signalgenerator_wavetrend(data_merged)\n",
    "signal_wt_df = data_merged.merge(wt_signal, left_index=True,right_index =True, how='inner')\n",
    "gainz = sum( abs(signal_wt_df['end_price']-signal_wt_df['start_price'])*10000 )\n",
    "print('---WaveTrend: '+ str(len(signal_wt_df.query('signal == wavetrend_signal'))) + ' out of: '+str(len(signal_wt_df))+ ' correct with: ' + str(round(gainz,5)) + ' pips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stochrsi generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srsi_signal = signalgenerator_stochrsi(data_merged)\n",
    "signal_srsi_df = data_merged.merge(srsi_signal, left_index=True,right_index =True, how='inner')\n",
    "gainz = sum( abs(signal_srsi_df['end_price']-signal_srsi_df['start_price'])*10000 )\n",
    "print('---WaveTrend: '+ str(len(signal_srsi_df.query('signal == stochrsi_signal'))) + ' out of: '+str(len(signal_srsi_df))+ ' correct with: ' + str(round(gainz,5)) + ' pips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mixed wavetrend and stochrsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_overall = data_merged.merge(wt_signal, left_index=True,right_index =True, how='left').merge(srsi_signal, left_index=True,right_index =True, how='left')\n",
    "signal_overall['stochrsi_signal'] = signal_overall['stochrsi_signal'].fillna(method='ffill')\n",
    "#signal_overall = signal_overall.dropna()\n",
    "signal_overall = signal_overall.query('wavetrend_signal == stochrsi_signal')\n",
    "print('---WaveTrend: '+ str(len(signal_overall.query('signal == wavetrend_signal'))) + ' out of: '+str(len(signal_overall))+ ' correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = signal_overall\n",
    "# Now let us look at the correlation coefficient of each of these variables\n",
    "x_cols = [col for col in df.columns if col not in ['OPEN','HIGH','LOW','CLOSE','start_price','end_price',\n",
    "                                                   'time_diff_hours','pct_change','weighted_target','signal','weight',]]\n",
    "\n",
    "target_col = 'signal'\n",
    "labels = []\n",
    "values = []\n",
    "for col in x_cols:\n",
    "    labels.append(col)\n",
    "    values.append(np.corrcoef(df[col].values, df[target_col].values)[0,1])\n",
    "    \n",
    "ind = np.arange(len(labels))\n",
    "width = 0.9\n",
    "fig, ax = plt.subplots(figsize=(16,16))\n",
    "plt.grid(True)\n",
    "rects = ax.barh(ind, np.array(values), color='y')\n",
    "ax.set_yticks(ind+((width)/2.))\n",
    "ax.set_yticklabels(labels, rotation='horizontal')\n",
    "ax.set_xlabel(\"Correlation coefficient\")\n",
    "ax.set_title(\"Correlation coefficient\")\n",
    "#autolabel(rects)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
